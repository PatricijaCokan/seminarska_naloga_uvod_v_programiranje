{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e418807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c95f9",
   "metadata": {},
   "source": [
    "AVTO.NET – ANALIZA OGLASOV\n",
    "\n",
    "SEMINARSKA NALOGA PRI PREDMETU UVOD V PROGRAMIRANJE 2024/2025\n",
    "\n",
    "PATRICIJA COKAN\n",
    "\n",
    "V svoji seminarski nalogi sem analizirala podatke o avtomobilskih oglasih. Oglase sem s pomočjo knjižnic requests in BeautifulSoup pridobila iz spletne strani avto.net. Vsi podatki, ki jih uporabljam v svojem projektu, so javne narave in dostopni na povezavi https://www.avto.net. Podatke sem za namen analize pretvorila v dve obliki zapisa, in sicer .json in .csv, tako kot je bilo prikazano na posnetkih vaj, ki sem si jih ogledala v spletni učilnici.\n",
    "Kot se je izkazalo, ima spletna stran avto.net kar nekaj zaščit proti spletnemu pajkanju (web scraping). Ko sem prvič poskusila pridobiti HTML-kodo posamezne strani, sem dobila le odgovor z opozorilom, da me je stran blokirala. Težavo sem nato delno rešila tako, da sem v svojo Python kodo dodala spremenljivko headers in jo uporabila pri pošiljanju zahtevkov na spletno stran. Poleg tega sem dodala še naključno čakanje med posameznimi zahtevki, kar je rezultate dodatno izboljšalo.\n",
    "Po teh spremembah sem uspela pridobiti kar nekaj veljavnih strani, vendar jih je bilo še vedno veliko neveljavnih. Veljavnost strani sem določala glede na količino podatkov: pri manjših straneh je šlo za napako, pri večjih pa za veljavne strani. V mapo s podatki sem tako shranjevala le veljavne strani. Ko sem kodo pognala večkrat, so nekatere strani, ki so bile prej neveljavne, postale veljavne in sem jih lahko dodala v zbirko podatkov. Na ta način sem z večkratnim poganjanjem kode uspela pridobiti veliko število veljavnih strani.\n",
    "Pri večkratnem poganjanju sem vsakič preverila, ali je določena stran že med veljavnimi podatki, in če je bila, sem jo preskočila. Tako sem se izognila morebitnemu ponovnemu branju strani, ki sem jih enkrat že uspešno prebrala.\n",
    "\n",
    "Ko sem pridobila zadostno količino strani, sem pripravila dve glavni Python funkciji. Prva funkcija iz vseh spletnih strani izloči posamezne oglase, ki jih najde na strani, druga pa iz teh oglasov pridobi podatke za vsak oglas posebej. Poleg teh dveh funkcij sem pripravila še funkcijo, ki očisti besedilo tako, da določene prej kodirane znake zamenja z dejanskimi črkami. Vse tri funkcije se izvajajo znotraj glavne (main) funkcije.\n",
    "\n",
    "Posamezen oglas za avto ima več atributov, in sicer: ime, cena, prva registracija, kilometrina, gorivo, menjalnik in motor. Podatke sem po osnovni analizi dodatno pretvorila v preglednejšo obliko, da so bili bolj primerni za nadaljnjo obdelavo. Odstranila sem odvečne presledke, razdelila ime avtomobila in znamko na dva ločena stolpca, odstranila enote ter jih zapisala kar v imenu stolpca ipd. Na ta način sem pridobila podatke, s katerimi je bilo nadaljnje delo enostavnejše in pri katerih sem imela manj težav s pretvorbo v pravilne podatkovne tipe, odstranjevanjem enot pri seštevanju podatkov ipd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
